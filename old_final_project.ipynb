{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm, skew, probplot\n",
    "from scipy.special import boxcox1p\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas.*\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file and see number of rows and cols\n",
    "nba_df = pd.read_csv(\"nba_2022-23_all_stats_with_salary.csv\")\n",
    "nba_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reanme 'Unnamed: 0' column to 'ID'\n",
    "nba_df = nba_df.rename(columns={\"Unnamed: 0\": \"Id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces from column names\n",
    "nba_df.columns = [col.replace(\" \", \"\") for col in nba_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of categorical variables\n",
    "category_count = 0\n",
    "\n",
    "for cat in nba_df.dtypes:\n",
    "    if cat == \"object\":\n",
    "        category_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of categorical variables:\", category_count)\n",
    "\n",
    "# column 1 is the ID column so we subract 1\n",
    "numeric_count = nba_df.shape[1] - category_count - 1\n",
    "\n",
    "print(\"Number of contineous variables:\", numeric_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all the column names\n",
    "nba_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling our missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the missing data and its percent of the column\n",
    "total_missing = nba_df.isnull().sum().sort_values(ascending=False)\n",
    "percent_missing = (nba_df.isnull().sum() / nba_df.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "missing_data_df = pd.concat([total_missing, percent_missing], axis=1, keys=[\"Total Missing\", \"Percent Missing\"])\n",
    "missing_data_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example row of a player who has missing data\n",
    "# players with missing data are those who did not play many games so they never accumilated that stat during the season\n",
    "null_fg = nba_df[nba_df['FG%'].isnull()]\n",
    "null_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize this in a bar graph\n",
    "missing_data_df[\"Percent Missing\"].head(8).plot(\n",
    "    kind=\"barh\", figsize=(20,10)\n",
    ").invert_yaxis()\n",
    "plt.xlabel(\"Percent Missing\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.title(\"The 8 Columns and their Percent of Missing Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing data with 0s\n",
    "# data is \"missing\" because player never recorded that stat during the season so we impute that data to be 0 to identify them in our model\n",
    "cols_to_fill_zero = [\n",
    "    \"FT%\",\n",
    "    \"3P%\",\n",
    "    \"2P%\",\n",
    "    \"TS%\",\n",
    "    \"3PAr\",\n",
    "    \"FTr\",\n",
    "    \"eFG%\",\n",
    "    \"FG%\",\n",
    "]\n",
    "\n",
    "for col in cols_to_fill_zero:\n",
    "    nba_df[col] = nba_df[col].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show same player who had null values now has zeros in those fields\n",
    "imputed_row = nba_df[nba_df[\"PlayerName\"] == \"Alondes Williams\"]\n",
    "imputed_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers for better training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=nba_df[\"GP\"], y=nba_df[\"Salary\"])\n",
    "plt.ylabel(\"Salary\", fontsize=13)\n",
    "plt.xlabel(\"GP (Games Played)\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be some outliers where players did not play the majority of the season, yet were given large salaries. This is likely due to season ending injuries. Additionally, there are players present in the data set that were on 10-day contracts. For this reason, we will remove data from players who played in less than 20 games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop less than 20 games\n",
    "nba_df = nba_df[nba_df['GP'] >= 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(nba_df[\"Salary\"], fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(nba_df[\"Salary\"])\n",
    "print(\"\\n mu = {:.2f} and sigma = {:.2f}\\n\".format(mu, sigma))\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.legend(\n",
    "    [\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )\".format(mu, sigma)], loc=\"best\"\n",
    ")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Salary distribution\")\n",
    "\n",
    "# Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = probplot(nba_df[\"Salary\"], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "nba_df[\"Salary_normalized\"] = np.log1p(nba_df[\"Salary\"])\n",
    "\n",
    "# Check the new distribution\n",
    "sns.distplot(nba_df[\"Salary_normalized\"], fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(nba_df[\"Salary_normalized\"])\n",
    "print(\"\\n mu = {:.2f} and sigma = {:.2f}\\n\".format(mu, sigma))\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.legend(\n",
    "    [\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )\".format(mu, sigma)], loc=\"best\"\n",
    ")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Salary distribution\")\n",
    "\n",
    "# Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = probplot(nba_df[\"Salary_normalized\"], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot\n",
    "sns.set()\n",
    "cols = [\n",
    "    \"Salary_normalized\",\n",
    "    \"Age\",\n",
    "    \"MP\",\n",
    "    \"3P\",\n",
    "    \"TRB\",\n",
    "    \"AST\",\n",
    "    \"PTS\",\n",
    "    \"PER\",\n",
    "    \"TS%\",\n",
    "    \"DWS\",\n",
    "    \"VORP\"\n",
    "]\n",
    "sns.pairplot(nba_df[cols], size=2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude non-numeric columns\n",
    "numeric_df = nba_df.select_dtypes(include=[np.number])\n",
    "corrmat = numeric_df.corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "sns.heatmap(corrmat, vmax=0.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_correlations = corrmat['Salary']\n",
    "print(salary_correlations.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize number of players at each position by age\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Age',hue='Position', data=nba_df, palette='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Variable Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot to visualize the spread of salaries by each position\n",
    "sns.boxplot(x='Position', y='Salary', data=nba_df, palette='rainbow');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to show correclation between points and salaries by position as well\n",
    "# points has the highest positive correlation to salary as seen above\n",
    "sns.lmplot(y='Salary', x='PTS', data=nba_df, hue='Position', palette='Set1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare Salary to VORP.\n",
    "VORP is a box score estimate of the points per 100 team possessions that a player contributes above a replacement level player, translated to an average team and proportional to an 82 game season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='VORP',y='Salary_normalized',data=nba_df,color='purple');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare Salary to a defensive advanced statistic like DWS.\n",
    "DWS stands for Defensive Win Shares, which is a metric in the NBA that compares a player's defensive rating to the league average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='DWS', y='Salary_normalized', data=nba_df, hue='Position', palette='viridis', alpha=0.6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode categorical varibales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be using the columns, PlayerName, Team, or Position, in our predictive model, as they have little to no correlation to Salary. Thus, we will bypass encoding this data to simplify data management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Remove Highly Correlated Features\n",
    "First, we calculate the correlation matrix to identify pairs of features that are highly correlated (correlation coefficient > 0.8). Highly correlated features can introduce multicollinearity, which negatively impacts model performance. We keep only one feature from each pair of highly correlated features to avoid redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Find highly correlated features\n",
    "\n",
    "corr_threshold = 0.8  \n",
    "correlated_features = set()\n",
    "\n",
    "for i in range(len(corrmat.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corrmat.iloc[i, j]) > corr_threshold:\n",
    "            colname_i = corrmat.columns[i]\n",
    "            colname_j = corrmat.columns[j]\n",
    "            # Keep one feature and add the other to the set of correlated features to be dropped\n",
    "            if colname_i not in correlated_features:\n",
    "                correlated_features.add(colname_j)\n",
    "\n",
    "# Drop the correlated features\n",
    "numeric_df_filtered = numeric_df.drop(columns=correlated_features)\n",
    "numeric_df_filtered.drop(['Salary_normalized'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Lasso Regression for Additional Feature Selection\n",
    "Next, we apply Lasso regression, a linear model that includes L1 regularization. Lasso regression not only helps in reducing overfitting but also performs feature selection by shrinking less important feature coefficients to zero. We use LassoCV, which includes cross-validation to find the optimal regularization parameter.\n",
    "\n",
    "We then use SelectFromModel to identify and retain only the most significant features based on the Lasso regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO Regression for additional feature selection\n",
    "lasso = LassoCV()\n",
    "lasso.fit(numeric_df_filtered, nba_df['Salary'])\n",
    "\n",
    "# Use SelectFromModel to get selected features based on LASSO coefficients\n",
    "sfm = SelectFromModel(lasso, prefit=True)\n",
    "selected_features_lasso = numeric_df_filtered.columns[sfm.get_support()]\n",
    "\n",
    "# Convert to a DataFrame if needed\n",
    "selected_features_df = pd.DataFrame(list(selected_features_lasso), columns=['Selected_Features'])\n",
    "\n",
    "selected_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significance\n",
    "By combining correlation-based feature removal with Lasso regression, we ensure that our model is trained on the most relevant features, improving its predictive power and robustness. This process is significant for predicting NBA salaries because it allows the model to focus on the key factors that truly influence player salaries, leading to more accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify skewness\n",
    "skewed_feats = (\n",
    "    numeric_df\n",
    "    .apply(lambda x: skew(x.dropna()))\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({\"Skew\": skewed_feats})\n",
    "skewness.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness[\"Skew\"].head(10).plot(\n",
    "    kind=\"barh\", figsize=(20, 10)\n",
    ").invert_yaxis()  # top 10 skewed columns\n",
    "plt.xlabel(\"Skew\")\n",
    "plt.ylabel(\"Variable Name\")\n",
    "plt.title(\"Top 10 Skewed Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\n",
    "    \"There are {} skewed numerical features to Box Cox transform (normalize)\".format(\n",
    "        skewness.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_value_columns = numeric_df.columns[(numeric_df < 0).any()]\n",
    "\n",
    "# Print the list of column names\n",
    "print(\"Columns with negative values:\")\n",
    "print(negative_value_columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    # skip over columns that don't need transformation\n",
    "    # skip over columns that have negative values so that they don't become NULL when transforming\n",
    "    if feat not in [\n",
    "        \"Id\",\n",
    "        \"Salary\",\n",
    "        \"Salary_normalized\",\n",
    "        'OWS', \n",
    "        'WS', \n",
    "        'WS/48', \n",
    "        'OBPM', \n",
    "        'DBPM', \n",
    "        'BPM', \n",
    "        'VORP'\n",
    "    ]:\n",
    "        nba_df[feat] = boxcox1p(nba_df[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the box cot did not add any NULL values\n",
    "null_columns = nba_df.columns[nba_df.isnull().any()]\n",
    "null_count = nba_df[null_columns].isnull().sum()\n",
    "\n",
    "print(\"Column Name: NULL Count\")\n",
    "for i in range(0, len(null_columns)):\n",
    "    print(f\"{null_columns[i]}: {null_count[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ccb5ea80edbfae8c6780fa0b5909321291a27faa1189b515049edc570dffb73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
