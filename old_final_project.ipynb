{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm, skew, probplot\n",
    "from scipy.special import boxcox1p\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas.*\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file and see number of rows and cols\n",
    "nba_df = pd.read_csv(\"nba_2022-23_all_stats_with_salary.csv\")\n",
    "nba_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reanme 'Unnamed: 0' column to 'ID'\n",
    "nba_df = nba_df.rename(columns={\"Unnamed: 0\": \"Id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces from column names\n",
    "nba_df.columns = [col.replace(\" \", \"\") for col in nba_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of categorical variables\n",
    "category_count = 0\n",
    "\n",
    "for cat in nba_df.dtypes:\n",
    "    if cat == \"object\":\n",
    "        category_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of categorical variables:\", category_count)\n",
    "\n",
    "# column 1 is the ID column so we subract 1\n",
    "numeric_count = nba_df.shape[1] - category_count - 1\n",
    "\n",
    "print(\"Number of contineous variables:\", numeric_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all the column names\n",
    "nba_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling our missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the missing data and its percent of the column\n",
    "total_missing = nba_df.isnull().sum().sort_values(ascending=False)\n",
    "percent_missing = (nba_df.isnull().sum() / nba_df.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "missing_data_df = pd.concat([total_missing, percent_missing], axis=1, keys=[\"Total Missing\", \"Percent Missing\"])\n",
    "missing_data_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example row of a player who has missing data\n",
    "# players with missing data are those who did not play many games so they never accumilated that stat during the season\n",
    "null_fg = nba_df[nba_df['FG%'].isnull()]\n",
    "null_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize this in a bar graph\n",
    "missing_data_df[\"Percent Missing\"].head(8).plot(\n",
    "    kind=\"barh\", figsize=(20,10)\n",
    ").invert_yaxis()\n",
    "plt.xlabel(\"Percent Missing\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.title(\"The 8 Columns and their Percent of Missing Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing data with 0s\n",
    "# data is \"missing\" because player never recorded that stat during the season so we impute that data to be 0 to identify them in our model\n",
    "cols_to_fill_zero = [\n",
    "    \"FT%\",\n",
    "    \"3P%\",\n",
    "    \"2P%\",\n",
    "    \"TS%\",\n",
    "    \"3PAr\",\n",
    "    \"FTr\",\n",
    "    \"eFG%\",\n",
    "    \"FG%\",\n",
    "]\n",
    "\n",
    "for col in cols_to_fill_zero:\n",
    "    nba_df[col] = nba_df[col].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show same player who had null values now has zeros in those fields\n",
    "imputed_row = nba_df[nba_df[\"PlayerName\"] == \"Alondes Williams\"]\n",
    "imputed_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers for better training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=nba_df[\"GP\"], y=nba_df[\"Salary\"])\n",
    "plt.ylabel(\"Salary\", fontsize=13)\n",
    "plt.xlabel(\"GP (Games Played)\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be some outliers where players did not play the majority of the season, yet were given large salaries. This is likely due to season ending injuries. Additionally, there are players present in the data set that were on 10-day contracts. For this reason, we will remove data from players who played in less than 20 games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop less than 20 games\n",
    "nba_df = nba_df[nba_df['GP'] >= 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(nba_df[\"Salary\"], fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(nba_df[\"Salary\"])\n",
    "print(\"\\n mu = {:.2f} and sigma = {:.2f}\\n\".format(mu, sigma))\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.legend(\n",
    "    [\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )\".format(mu, sigma)], loc=\"best\"\n",
    ")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Salary distribution\")\n",
    "\n",
    "# Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = probplot(nba_df[\"Salary\"], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "nba_df[\"Salary_normalized\"] = np.log1p(nba_df[\"Salary\"])\n",
    "\n",
    "# Check the new distribution\n",
    "sns.distplot(nba_df[\"Salary_normalized\"], fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(nba_df[\"Salary_normalized\"])\n",
    "print(\"\\n mu = {:.2f} and sigma = {:.2f}\\n\".format(mu, sigma))\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.legend(\n",
    "    [\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )\".format(mu, sigma)], loc=\"best\"\n",
    ")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Salary distribution\")\n",
    "\n",
    "# Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = probplot(nba_df[\"Salary_normalized\"], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot\n",
    "sns.set()\n",
    "cols = [\n",
    "    \"Salary_normalized\",\n",
    "    \"Age\",\n",
    "    \"MP\",\n",
    "    \"3P\",\n",
    "    \"TRB\",\n",
    "    \"AST\",\n",
    "    \"PTS\",\n",
    "    \"PER\",\n",
    "    \"TS%\",\n",
    "    \"DWS\",\n",
    "    \"VORP\"\n",
    "]\n",
    "sns.pairplot(nba_df[cols], size=2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude non-numeric columns\n",
    "numeric_df = nba_df.select_dtypes(include=[np.number])\n",
    "corrmat = numeric_df.corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "sns.heatmap(corrmat, vmax=0.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_correlations = corrmat['Salary']\n",
    "print(salary_correlations.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize number of players at each position by age\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Age',hue='Position', data=nba_df, palette='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Variable Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot to visualize the spread of salaries by each position\n",
    "sns.boxplot(x='Position', y='Salary', data=nba_df, palette='rainbow');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to show correclation between points and salaries by position as well\n",
    "# points has the highest positive correlation to salary as seen above\n",
    "sns.lmplot(y='Salary', x='PTS', data=nba_df, hue='Position', palette='Set1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare Salary to VORP.\n",
    "VORP is a box score estimate of the points per 100 team possessions that a player contributes above a replacement level player, translated to an average team and proportional to an 82 game season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='VORP',y='Salary_normalized',data=nba_df,color='purple');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare Salary to a defensive advanced statistic like DWS.\n",
    "DWS stands for Defensive Win Shares, which is a metric in the NBA that compares a player's defensive rating to the league average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='DWS', y='Salary_normalized', data=nba_df, hue='Position', palette='viridis', alpha=0.6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode categorical varibales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be using the columns, PlayerName, Team, or Position, in our predictive model, as they have little to no correlation to Salary. Thus, we will bypass encoding this data to simplify data management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Remove Highly Correlated Features\n",
    "First, we calculate the correlation matrix to identify pairs of features that are highly correlated (correlation coefficient > 0.8). Highly correlated features can introduce multicollinearity, which negatively impacts model performance. We keep only one feature from each pair of highly correlated features to avoid redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Find highly correlated features\n",
    "\n",
    "corr_threshold = 0.8  \n",
    "correlated_features = set()\n",
    "\n",
    "for i in range(len(corrmat.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corrmat.iloc[i, j]) > corr_threshold:\n",
    "            colname_i = corrmat.columns[i]\n",
    "            colname_j = corrmat.columns[j]\n",
    "            # Keep one feature and add the other to the set of correlated features to be dropped\n",
    "            if colname_i not in correlated_features:\n",
    "                correlated_features.add(colname_j)\n",
    "\n",
    "# Drop the correlated features\n",
    "numeric_df_filtered = numeric_df.drop(columns=correlated_features)\n",
    "numeric_df_filtered.drop(['Salary_normalized'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Lasso Regression for Additional Feature Selection\n",
    "Next, we apply Lasso regression, a linear model that includes L1 regularization. Lasso regression not only helps in reducing overfitting but also performs feature selection by shrinking less important feature coefficients to zero. We use LassoCV, which includes cross-validation to find the optimal regularization parameter.\n",
    "\n",
    "We then use SelectFromModel to identify and retain only the most significant features based on the Lasso regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO Regression for additional feature selection\n",
    "lasso = LassoCV()\n",
    "lasso.fit(numeric_df_filtered, nba_df['Salary'])\n",
    "\n",
    "# Use SelectFromModel to get selected features based on LASSO coefficients\n",
    "sfm = SelectFromModel(lasso, prefit=True)\n",
    "selected_features_lasso = numeric_df_filtered.columns[sfm.get_support()]\n",
    "\n",
    "# Convert to a DataFrame if needed\n",
    "selected_features_df = pd.DataFrame(list(selected_features_lasso), columns=['Selected_Features'])\n",
    "\n",
    "selected_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significance\n",
    "By combining correlation-based feature removal with Lasso regression, we ensure that our model is trained on the most relevant features, improving its predictive power and robustness. This process is significant for predicting NBA salaries because it allows the model to focus on the key factors that truly influence player salaries, leading to more accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify skewness\n",
    "skewed_feats = (\n",
    "    numeric_df\n",
    "    .apply(lambda x: skew(x.dropna()))\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({\"Skew\": skewed_feats})\n",
    "skewness.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness[\"Skew\"].head(10).plot(\n",
    "    kind=\"barh\", figsize=(20, 10)\n",
    ").invert_yaxis()  # top 10 skewed columns\n",
    "plt.xlabel(\"Skew\")\n",
    "plt.ylabel(\"Variable Name\")\n",
    "plt.title(\"Top 10 Skewed Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\n",
    "    \"There are {} skewed numerical features to Box Cox transform (normalize)\".format(\n",
    "        skewness.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_value_columns = numeric_df.columns[(numeric_df < 0).any()]\n",
    "\n",
    "# Print the list of column names\n",
    "print(\"Columns with negative values:\")\n",
    "print(negative_value_columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    # skip over columns that don't need transformation\n",
    "    # skip over columns that have negative values so that they don't become NULL when transforming\n",
    "    if feat not in [\n",
    "        \"Id\",\n",
    "        \"Salary\",\n",
    "        \"Salary_normalized\",\n",
    "        'OWS', \n",
    "        'WS', \n",
    "        'WS/48', \n",
    "        'OBPM', \n",
    "        'DBPM', \n",
    "        'BPM', \n",
    "        'VORP'\n",
    "    ]:\n",
    "        nba_df[feat] = boxcox1p(nba_df[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the box cot did not add any NULL values\n",
    "null_columns = nba_df.columns[nba_df.isnull().any()]\n",
    "null_count = nba_df[null_columns].isnull().sum()\n",
    "\n",
    "print(\"Column Name: NULL Count\")\n",
    "for i in range(0, len(null_columns)):\n",
    "    print(f\"{null_columns[i]}: {null_count[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our data is labled therefore we will be implementing supervised learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_compute_rmse(df, model):\n",
    "    # Splitting the data into train and test set  \n",
    "    X = df.drop(columns=['Salary', 'Id', 'PlayerName', 'Position', 'Team', 'Salary_normalized'])\n",
    "    y = df['Salary']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Training the model\n",
    "    model.fit(X_train, np.log1p(y_train))\n",
    "    \n",
    "    # Making predictions on the test set\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    if (\n",
    "            np.isnan(y_pred).any()\n",
    "            or np.isinf(y_pred).any()\n",
    "        ):\n",
    "            print(\n",
    "                f\"Warning: NaN or infinity values found in predictions or true values. Imputing 0 for problematic values in y_pred.\"\n",
    "            )\n",
    "            y_pred[np.isnan(y_pred) | np.isinf(y_pred)] = 0\n",
    "\n",
    "    # Computing RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_std(df, model):\n",
    "    rmse_list = []\n",
    "    for i in range(30):\n",
    "        rmse_list.append(train_and_compute_rmse(df, model))\n",
    "\n",
    "    mean = np.mean(rmse_list)\n",
    "    std = np.std(rmse_list)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_w_int = LinearRegression()\n",
    "lr_no_int = LinearRegression(fit_intercept=False)\n",
    "elastic_net = ElasticNet(alpha=0.01, l1_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsRegressor(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(max_depth=5, n_estimators=1000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMRegressor(max_depth=5, n_estimators=1000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the avg rmse and std over 30 tests for each model\n",
    "lr_no_int_list = compute_rmse_std(nba_df, lr_no_int)\n",
    "lr_w_int_list = compute_rmse_std(nba_df, lr_w_int)\n",
    "elastic_net_list = compute_rmse_std(nba_df, elastic_net)\n",
    "#neigh_list = compute_rmse_std(nba_df, neigh)\n",
    "dt_list = compute_rmse_std(nba_df, dt)\n",
    "rf_list = compute_rmse_std(nba_df, rf)\n",
    "model_xgb_list = compute_rmse_std(nba_df, model_xgb)\n",
    "\n",
    "# plot RMSE and STD for each Algorithm\n",
    "data = {\n",
    "    \"Linear (No Intercept)\": lr_no_int_list,\n",
    "    \"Linear (w/ Intercept)\": lr_w_int_list,\n",
    "    \"Elastic Net\": elastic_net_list,\n",
    "    #\"Nearest Neighbor\": neigh_list,\n",
    "    \"Decision Tree\": dt_list,\n",
    "    \"Random Forest\": rf_list,\n",
    "    \"XGBoost\": model_xgb_list,\n",
    "}\n",
    "data_df = pd.DataFrame(data=data).T.reset_index().sort_values(by=[0], ascending=True)\n",
    "data_df.columns = [\"Algorithm\", \"RMSE\", \"STD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the bar plot\n",
    "data_df.plot(kind=\"bar\", x=\"Algorithm\", y=[\"RMSE\", \"STD\"], figsize=(20, 10), rot=0)\n",
    "plt.xlabel(\"Algorithm\", fontsize=20)\n",
    "plt.ylabel(\"Root Mean Squared Error / Standard Deviation\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tune_bayesian(X_train, y_train, regressor):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for XGBoost or LightGBM using Bayesian search.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: pandas DataFrame\n",
    "        Training features.\n",
    "    - y_train: pandas Series\n",
    "        Training target variable.\n",
    "    - regressor_type: str\n",
    "        Type of regressor to tune ('xgboost' or 'lightgbm').\n",
    "\n",
    "    Returns:\n",
    "    - best_params: dict\n",
    "        Best hyperparameters found during tuning.\n",
    "    \"\"\"\n",
    "    # Define the common parameter space for both XGBoost and LightGBM\n",
    "    param_space_common = {\n",
    "        \"n_estimators\": (100, 1200),\n",
    "        \"learning_rate\": (0.01, 0.2, \"log-uniform\"),\n",
    "        \"max_depth\": (3, 10),\n",
    "    }\n",
    "\n",
    "    regressor_type = regressor.lower()\n",
    "    if regressor_type == \"xgboost\":\n",
    "        regressor = xgb.XGBRegressor()\n",
    "    elif regressor_type == \"lightgbm\":\n",
    "        regressor = lgb.LGBMRegressor()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported regressor type. Choose 'xgboost' or 'lightgbm'.\")\n",
    "\n",
    "    # Update the search space with common parameters\n",
    "    param_space = param_space_common.copy()\n",
    "\n",
    "    # Perform Bayesian search\n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator=regressor,\n",
    "        search_spaces=param_space,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=5,\n",
    "        n_jobs=-1,  # Set the number of parallel jobs\n",
    "    )\n",
    "    bayes_search.fit(X_train, np.log1p(y_train))\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = bayes_search.best_params_\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_regression(\n",
    "    data,\n",
    "    regressor,\n",
    "    target_column=\"Salary\",\n",
    "    cols_to_ignore=['Salary', 'Id', 'PlayerName', 'Position', 'Team', 'Salary_normalized'],\n",
    "    n_splits=5,\n",
    "    tune_hyperparameters=False,\n",
    "):\n",
    "    rmse_scores = []\n",
    "    train_sizes = []\n",
    "    test_sizes = []\n",
    "\n",
    "    X = data.drop(columns=cols_to_ignore)\n",
    "    y = data[target_column]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True) \n",
    "\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        if (\n",
    "            isinstance(regressor, (xgb.XGBRegressor, lgb.LGBMRegressor))\n",
    "            and tune_hyperparameters\n",
    "        ):  # Add LGBMRegressor to the isinstance check\n",
    "            # Determine the regressor_type based on the type of the regressor\n",
    "            if isinstance(regressor, xgb.XGBRegressor):\n",
    "                regressor_type = \"XGBoost\"\n",
    "            elif isinstance(regressor, lgb.LGBMRegressor):\n",
    "                regressor_type = \"LightGBM\"\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Unsupported regressor type. Supported types: XGBRegressor, LGBMRegressor\"\n",
    "                )\n",
    "\n",
    "            X_train_hyper, y_train_hyper = (\n",
    "                data.drop(cols_to_ignore, axis=1),\n",
    "                data[target_column],\n",
    "            )\n",
    "            best_params = hyperparameter_tune_bayesian(\n",
    "                X_train_hyper, y_train_hyper, regressor_type\n",
    "            )  # Specify 'xgboost' or 'lightgm' as the regressor type\n",
    "            print(\n",
    "                f\"Best hyperparameters for {regressor_type} Fold: {best_params}\"\n",
    "            )\n",
    "            regressor.set_params(**best_params)  # Set the best hyperparameters\n",
    "\n",
    "\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        regressor.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = regressor.predict(X_val_fold)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "        rmse_scores.append(rmse)\n",
    "        train_sizes.append(len(y_train_fold))\n",
    "        test_sizes.append(len(y_val_fold))\n",
    "\n",
    "\n",
    "    # Print RMSE scores for each fold\n",
    "    # for fold, rmse in enumerate(rmse_scores):\n",
    "    #     print(f\"Fold {fold+1} RMSE: {rmse}\")\n",
    "\n",
    "    return rmse_scores, train_sizes, test_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_std_k_fold(df, model, tune_hyper=False):\n",
    "    rmse_list = []\n",
    "        \n",
    "    rmse_list.append(k_fold_regression(df, model, tune_hyperparameters=tune_hyper))\n",
    "\n",
    "    mean = np.mean(rmse_list)\n",
    "    std = np.std(rmse_list)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rmse_k_fold(data, regressor, target_column=\"Salary\", cols_to_ignore=['Salary', 'Id', 'PlayerName', 'Position', 'Team', 'Salary_normalized'], n_splits=5, tune_hyperparameters=False):\n",
    "    rmse_scores, train_sizes, test_sizes = k_fold_regression(\n",
    "        data, regressor, target_column, cols_to_ignore, n_splits, tune_hyperparameters\n",
    "    )\n",
    "\n",
    "    for fold, (rmse, train_size, test_size) in enumerate(zip(rmse_scores, train_sizes, test_sizes)):\n",
    "        print(f\"Fold {fold + 1} RMSE: {rmse:.4f}, Train Size: {train_size}, Test Size: {test_size}\")\n",
    "\n",
    "    overall_rmse = np.mean(rmse_scores)\n",
    "    print(f\"Overall RMSE: {overall_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the avg rmse and std over 30 tests for each model\n",
    "lr_no_int_list = compute_rmse_std_k_fold(nba_df, lr_no_int)\n",
    "lr_w_int_list = compute_rmse_std_k_fold(nba_df, lr_w_int)\n",
    "elastic_net_list = compute_rmse_std_k_fold(nba_df, elastic_net)\n",
    "#neigh_list = compute_rmse_std_k_fold(nba_df, neigh)\n",
    "dt_list = compute_rmse_std_k_fold(nba_df, dt)\n",
    "rf_list = compute_rmse_std_k_fold(nba_df, rf)\n",
    "model_xgb_list = compute_rmse_std_k_fold(nba_df, model_xgb)\n",
    "\n",
    "#The next line takes a while (Roughly 10 mins), If want to quickly run, comment out this line and the line below in data\n",
    "#model_xgb_hyper_list = compute_rmse_std_k_fold(nba_df, model_xgb, tune_hyper=True)\n",
    "\n",
    "# plot RMSE and STD for each Algorithm\n",
    "data = {\n",
    "    \"Linear (No Intercept)\": lr_no_int_list,\n",
    "    \"Linear (w/ Intercept)\": lr_w_int_list,\n",
    "    \"Elastic Net\": elastic_net_list,\n",
    "    #\"Nearest Neighbor\": neigh_list,\n",
    "    \"Decision Tree\": dt_list,\n",
    "    \"Random Forest\": rf_list,\n",
    "    \"XGBoost\": model_xgb_list,\n",
    "\n",
    "    #Comment below if want to run quicker\n",
    "    #\"XGBoost Hyper\": model_xgb_hyper_list,\n",
    "}\n",
    "data_df = pd.DataFrame(data=data).T.reset_index().sort_values(by=[0], ascending=True)\n",
    "data_df.columns = [\"Algorithm\", \"RMSE\", \"STD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the bar plot\n",
    "data_df.plot(kind=\"bar\", x=\"Algorithm\", y=[\"RMSE\", \"STD\"], figsize=(20, 10), rot=0)\n",
    "plt.xlabel(\"Algorithm\", fontsize=20)\n",
    "plt.ylabel(\"Root Mean Squared Error / Standard Deviation\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# first stacking model\n",
    "  \n",
    "estimators = [\n",
    "   ('elastic_net', elastic_net),\n",
    "   ('model_xgb', model_xgb),\n",
    "   ('lr_w_int', lr_w_int)\n",
    "]\n",
    "\n",
    "\n",
    "sr = StackingRegressor(\n",
    "   estimators=estimators,\n",
    "   final_estimator=rf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# voting stacking model, putting weights on different models\n",
    "\n",
    "vr = VotingRegressor([\n",
    "   ('rf', rf),\n",
    "   ('model_xgb', model_xgb),\n",
    "   ('elastic_net', elastic_net),\n",
    "   ('lr_w_int', lr_w_int)\n",
    "  \n",
    "], weights=[1,2,2,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators2 = [\n",
    "   ('elastic_net', elastic_net),\n",
    "   ('model_xgb', model_xgb),\n",
    "   ('lr_w_int', lr_w_int),\n",
    "   ('rf', rf)\n",
    "]\n",
    "\n",
    "# using the voting model as our final estimator\n",
    "\n",
    "sr2 = StackingRegressor(\n",
    "   estimators=estimators2,\n",
    "   final_estimator=vr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More tesing with the new models\n",
    "Removed Nearest Neighbor, Linear No Int, Linear W Int, Elastic Net, and Descision tree because they are worst performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the avg rmse and std over 30 tests for each model\n",
    "rf_list = compute_rmse_std_k_fold(nba_df, rf)\n",
    "model_xgb_list = compute_rmse_std_k_fold(nba_df, model_xgb)\n",
    "sr_list = compute_rmse_std_k_fold(nba_df, sr)\n",
    "vr_list = compute_rmse_std_k_fold(nba_df, vr)\n",
    "sr2_list = compute_rmse_std_k_fold(nba_df, sr2)\n",
    "model_xgb_hyper_list = compute_rmse_std_k_fold(nba_df, model_xgb, tune_hyper=True)\n",
    "sr_hyper_list = compute_rmse_std_k_fold(nba_df, sr, tune_hyper=True)\n",
    "vr_hyper_list = compute_rmse_std_k_fold(nba_df, vr, tune_hyper=True)\n",
    "sr2_hyper_list = compute_rmse_std_k_fold(nba_df, sr2, tune_hyper=True)\n",
    "\n",
    "\n",
    "# plot RMSE and STD for each Algorithm\n",
    "data = {\n",
    "    \"Random Forest\": rf_list,\n",
    "    \"XGBoost\": model_xgb_list,\n",
    "    \"Stacking Regressor\": sr_list,\n",
    "    \"Voting Regressor\": vr_list,\n",
    "    \"Stacking Regressor 2\": sr2_list,\n",
    "    \"XGBoost Hyper\": model_xgb_hyper_list,\n",
    "    \"Stacking Regressor Hyper\": sr_hyper_list,\n",
    "    \"Voting Regressor Hyper\": vr_hyper_list,\n",
    "    \"Stacking Regressor 2 Hyper\": sr2_hyper_list,\n",
    "}\n",
    "data_df = pd.DataFrame(data=data).T.reset_index().sort_values(by=[0], ascending=True)\n",
    "data_df.columns = [\"Algorithm\", \"RMSE\", \"STD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the bar plot\n",
    "data_df.plot(kind=\"bar\", x=\"Algorithm\", y=[\"RMSE\", \"STD\"], figsize=(20, 10), rot=0)\n",
    "plt.xlabel(\"Algorithm\", fontsize=20)\n",
    "plt.ylabel(\"Root Mean Squared Error / Standard Deviation\", fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ccb5ea80edbfae8c6780fa0b5909321291a27faa1189b515049edc570dffb73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
