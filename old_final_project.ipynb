{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm, skew, probplot\n",
    "from scipy.special import boxcox1p\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas.*\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file and see number of rows and cols\n",
    "nba_df = pd.read_csv(\"nba_2022-23_all_stats_with_salary.csv\")\n",
    "nba_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are grabbing salaries from the past 5 seasons from another dataset and adding them as columns in our nba_df. We are not using salary data from the last season becuase it would be too highly correlated with the salary data we are trying to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_nba_salaries = pd.read_csv('full_nba_salaries.csv')\n",
    "\n",
    "# Define the years and corresponding column names for salaries\n",
    "years = ['2017-18', '2018-19', '2019-20', '2020-21']\n",
    "salary_columns = [f'Salary{year[2:4]}-{year[5:]}' for year in years]\n",
    "\n",
    "# Initialize new salary columns in the nba_stats DataFrame with 0\n",
    "for col in salary_columns:\n",
    "    nba_df[col] = 0\n",
    "\n",
    "# Iterate through the full_nba_salaries DataFrame\n",
    "for index, row in full_nba_salaries.iterrows():\n",
    "    season = row['Season']\n",
    "    player_name = row['Name']\n",
    "    \n",
    "    if season in years:\n",
    "        salary_col = f'Salary{season[2:4]}-{season[5:]}'\n",
    "        # Update the salary in the nba_stats DataFrame, converting to integer\n",
    "        nba_df.loc[nba_df['Player Name'] == player_name, salary_col] = int(row['Salary'].replace('$', '').replace(',', '').replace('(TW)', ''))\n",
    "\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "nba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reanme 'Unnamed: 0' column to 'ID'\n",
    "nba_df = nba_df.rename(columns={\"Unnamed: 0\": \"Id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces from column names\n",
    "nba_df.columns = [col.replace(\" \", \"\") for col in nba_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of categorical variables\n",
    "category_count = 0\n",
    "\n",
    "for cat in nba_df.dtypes:\n",
    "    if cat == \"object\":\n",
    "        category_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of categorical variables:\", category_count)\n",
    "\n",
    "# column 1 is the ID column so we subract 1\n",
    "numeric_count = nba_df.shape[1] - category_count - 1\n",
    "\n",
    "print(\"Number of contineous variables:\", numeric_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all the column names\n",
    "nba_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling our missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the missing data and its percent of the column\n",
    "total_missing = nba_df.isnull().sum().sort_values(ascending=False)\n",
    "percent_missing = (nba_df.isnull().sum() / nba_df.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "missing_data_df = pd.concat([total_missing, percent_missing], axis=1, keys=[\"Total Missing\", \"Percent Missing\"])\n",
    "missing_data_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example row of a player who has missing data\n",
    "# players with missing data are those who did not play many games so they never accumilated that stat during the season\n",
    "null_fg = nba_df[nba_df['FG%'].isnull()]\n",
    "null_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize this in a bar graph\n",
    "missing_data_df[\"Percent Missing\"].head(8).plot(\n",
    "    kind=\"barh\", figsize=(20,10)\n",
    ").invert_yaxis()\n",
    "plt.xlabel(\"Percent Missing\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.title(\"The 8 Columns and their Percent of Missing Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing data with 0s\n",
    "# data is \"missing\" because player never recorded that stat during the season so we impute that data to be 0 to identify them in our model\n",
    "cols_to_fill_zero = [\n",
    "    \"FT%\",\n",
    "    \"3P%\",\n",
    "    \"2P%\",\n",
    "    \"TS%\",\n",
    "    \"3PAr\",\n",
    "    \"FTr\",\n",
    "    \"eFG%\",\n",
    "    \"FG%\",\n",
    "]\n",
    "\n",
    "for col in cols_to_fill_zero:\n",
    "    nba_df[col] = nba_df[col].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show same player who had null values now has zeros in those fields\n",
    "imputed_row = nba_df[nba_df[\"PlayerName\"] == \"Alondes Williams\"]\n",
    "imputed_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers for better training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=nba_df[\"GP\"], y=nba_df[\"Salary\"])\n",
    "plt.ylabel(\"Salary\", fontsize=13)\n",
    "plt.xlabel(\"GP (Games Played)\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be some outliers where players did not play the majority of the season, yet were given large salaries. This is likely due to season ending injuries. Additionally, there are players present in the data set that were on 10-day contracts. For this reason, we will remove data from players who played in less than 20 games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop less than 20 games\n",
    "nba_df = nba_df[nba_df['GP'] >= 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(nba_df[\"Salary\"], fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(nba_df[\"Salary\"])\n",
    "print(\"\\n mu = {:.2f} and sigma = {:.2f}\\n\".format(mu, sigma))\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.legend(\n",
    "    [\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )\".format(mu, sigma)], loc=\"best\"\n",
    ")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Salary distribution\")\n",
    "\n",
    "# Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = probplot(nba_df[\"Salary\"], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "nba_df[\"Salary_normalized\"] = np.log1p(nba_df[\"Salary\"])\n",
    "\n",
    "# Check the new distribution\n",
    "sns.distplot(nba_df[\"Salary_normalized\"], fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(nba_df[\"Salary_normalized\"])\n",
    "print(\"\\n mu = {:.2f} and sigma = {:.2f}\\n\".format(mu, sigma))\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.legend(\n",
    "    [\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )\".format(mu, sigma)], loc=\"best\"\n",
    ")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Salary distribution\")\n",
    "\n",
    "# Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = probplot(nba_df[\"Salary_normalized\"], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot\n",
    "sns.set()\n",
    "cols = [\n",
    "    \"Salary_normalized\",\n",
    "    \"Age\",\n",
    "    \"MP\",\n",
    "    \"3P\",\n",
    "    \"TRB\",\n",
    "    \"AST\",\n",
    "    \"PTS\",\n",
    "    \"PER\",\n",
    "    \"TS%\",\n",
    "    \"DWS\",\n",
    "    \"VORP\"\n",
    "]\n",
    "sns.pairplot(nba_df[cols], size=2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude non-numeric columns\n",
    "numeric_df = nba_df.select_dtypes(include=[np.number])\n",
    "corrmat = numeric_df.corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "sns.heatmap(corrmat, vmax=0.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_correlations = corrmat['Salary_normalized']\n",
    "print(salary_correlations.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize number of players at each position by age\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Age',hue='Position', data=nba_df, palette='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Variable Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot to visualize the spread of salaries by each position\n",
    "sns.boxplot(x='Position', y='Salary_normalized', data=nba_df, palette='rainbow');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to show correclation between points and salaries by position as well\n",
    "# points has the highest positive correlation to salary as seen above\n",
    "sns.lmplot(y='Salary', x='PTS', data=nba_df, hue='Position', palette='Set1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare Salary to VORP.\n",
    "VORP is a box score estimate of the points per 100 team possessions that a player contributes above a replacement level player, translated to an average team and proportional to an 82 game season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='VORP',y='Salary_normalized',data=nba_df,color='purple');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare Salary to a defensive advanced statistic like DWS.\n",
    "DWS stands for Defensive Win Shares, which is a metric in the NBA that compares a player's defensive rating to the league average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='DWS', y='Salary_normalized', data=nba_df, hue='Position', palette='viridis', alpha=0.6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Dropping low correlated features\n",
    "First, we are dropping features that have below a 0.2 correlation index with 'Salary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the columns to exclude from feature selection\n",
    "exclude_columns = ['Id', 'Salary', 'Salary_normalized']\n",
    "\n",
    "# Get the numerical columns excluding the columns to exclude\n",
    "numeric_columns = [col for col in numeric_df.columns if col not in exclude_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_corr_columns = [col for col in numeric_columns if abs(corrmat.loc[col, 'Salary']) < 0.2]\n",
    "\n",
    "print(low_corr_columns)\n",
    "\n",
    "numeric_df.drop(columns=low_corr_columns, inplace=True)\n",
    "\n",
    "nba_df.drop(columns=low_corr_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Encoding Categorical Features\n",
    "Next, we will encode categorical features so that our supervised model can use them for predicitons. These features are Position and Team. The position and team are likely influential on a player's salary but are represented by strings in our dataset. We will label encoding because the values of these features are within a limited range. This will give each team and position a unique numerical marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create label encoders\n",
    "position_encoder = LabelEncoder()\n",
    "team_encoder = LabelEncoder()\n",
    "\n",
    "# Perform label encoding for 'Position' and 'Team' variables\n",
    "nba_df['Position_encoded'] = position_encoder.fit_transform(nba_df['Position'])\n",
    "nba_df['Team_encoded'] = team_encoder.fit_transform(nba_df['Team'])\n",
    "\n",
    "# Drop the original 'Position' and 'Team' columns\n",
    "nba_df.drop(['Position', 'Team'], axis=1, inplace=True)\n",
    "nba_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify skewness\n",
    "skewed_feats = (\n",
    "    numeric_df\n",
    "    .apply(lambda x: skew(x.dropna()))\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({\"Skew\": skewed_feats})\n",
    "skewness.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness[\"Skew\"].head(10).plot(\n",
    "    kind=\"barh\", figsize=(20, 10)\n",
    ").invert_yaxis()  # top 10 skewed columns\n",
    "plt.xlabel(\"Skew\")\n",
    "plt.ylabel(\"Variable Name\")\n",
    "plt.title(\"Top 10 Skewed Variables\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "print(\n",
    "    \"There are {} skewed numerical features to Box Cox transform (normalize)\".format(\n",
    "        skewness.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_value_columns = numeric_df.columns[(numeric_df < 0).any()]\n",
    "\n",
    "# Print the list of column names\n",
    "print(\"Columns with negative values:\")\n",
    "print(negative_value_columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    # skip over columns that don't need transformation\n",
    "    # skip over columns that have negative values so that they don't become NULL when transforming\n",
    "    if feat not in [\n",
    "        \"Id\",\n",
    "        \"Salary\",\n",
    "        \"Salary_normalized\",\n",
    "        \"Salary17-18\",\n",
    "        \"Salary18-19\",\n",
    "        \"Salary19-20\",\n",
    "        \"Salary20-21\",\n",
    "        \"Salary21-22\",\n",
    "        'OWS', \n",
    "        'WS', \n",
    "        'WS/48', \n",
    "        'OBPM', \n",
    "        'DBPM', \n",
    "        'BPM', \n",
    "        'VORP'\n",
    "    ]:\n",
    "        nba_df[feat] = boxcox1p(nba_df[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the box cot did not add any NULL values\n",
    "null_columns = nba_df.columns[nba_df.isnull().any()]\n",
    "null_count = nba_df[null_columns].isnull().sum()\n",
    "\n",
    "print(\"Column Name: NULL Count\")\n",
    "for i in range(0, len(null_columns)):\n",
    "    print(f\"{null_columns[i]}: {null_count[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our data is labled therefore we will be implementing supervised learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define multiple regression features that will train and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_w_int = LinearRegression()\n",
    "lr_no_int = LinearRegression(fit_intercept=False)\n",
    "elastic_net = ElasticNet(alpha=0.01, l1_ratio=0.1)\n",
    "rf = RandomForestRegressor(n_estimators=500)\n",
    "dt = DecisionTreeRegressor(max_depth=10)\n",
    "model_xgb = xgb.XGBRegressor(max_depth=5, n_estimators=1000, learning_rate=0.01)\n",
    "model_xgb_hyper = xgb.XGBRegressor(max_depth=5, n_estimators=1000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, `hyperparameter_tune_bayesian`, performs Bayesian hyperparameter tuning for an XGBoost regressor using training features (`X_train`) and target variable (`y_train`). It defines a parameter search space, applies Bayesian optimization with cross-validation, and returns the best hyperparameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tune_bayesian(X_train, y_train, regressor):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for XGBoost using Bayesian search.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: pandas DataFrame\n",
    "        Training features.\n",
    "    - y_train: pandas Series\n",
    "        Training target variable.\n",
    "    - regressor_type: str\n",
    "        Type of regressor to tune ('xgboost').\n",
    "\n",
    "    Returns:\n",
    "    - best_params: dict\n",
    "        Best hyperparameters found during tuning.\n",
    "    \"\"\"\n",
    "    # Define the common parameter space for both XGBoost\n",
    "    param_space_common = {\n",
    "        \"n_estimators\": (100, 1200),\n",
    "        \"learning_rate\": (0.01, 0.2, \"log-uniform\"),\n",
    "        \"max_depth\": (3, 10),\n",
    "    }\n",
    "\n",
    "    regressor_type = regressor.lower()\n",
    "    if regressor_type == \"xgboost\":\n",
    "        regressor = xgb.XGBRegressor()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported regressor type. Choose 'xgboost'.\")\n",
    "\n",
    "    # Update the search space with common parameters\n",
    "    param_space = param_space_common.copy()\n",
    "\n",
    "    # Perform Bayesian search\n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator=regressor,\n",
    "        search_spaces=param_space,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=5,\n",
    "        n_jobs=-1,  # Set the number of parallel jobs\n",
    "    )\n",
    "    bayes_search.fit(X_train, np.log1p(y_train))\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = bayes_search.best_params_\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, `k_fold_regression`, performs k-fold cross-validation on a given dataset using a specified regressor to predict a target column (default \"Salary\"). It splits the data into training and validation sets, fits the model, and evaluates its performance using RMSE for each fold. Optionally, it can tune hyperparameters for an XGBoost regressor. The function returns a DataFrame with prediction results for each fold, along with lists of RMSE scores, training set sizes, and validation set sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_regression(\n",
    "    data,\n",
    "    regressor,\n",
    "    target_column=\"Salary\",\n",
    "    cols_to_ignore=['Salary', 'Id', 'PlayerName', 'Salary_normalized'],\n",
    "    n_splits=5,\n",
    "    tune_hyperparameters=False,\n",
    "    model_name=None\n",
    "):\n",
    "    rmse_scores = []\n",
    "    train_sizes = []\n",
    "    test_sizes = []\n",
    "    fold_results = []\n",
    "\n",
    "    # Prepare the feature matrix X and target vector y\n",
    "    X = data.drop(columns=cols_to_ignore)\n",
    "    y = data[target_column]\n",
    "\n",
    "    # Initialize KFold cross-validator\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "    # Cross-validation process\n",
    "    foldCount = 0\n",
    "    fold_models = []\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        if (\n",
    "            isinstance(regressor, (xgb.XGBRegressor))\n",
    "            and tune_hyperparameters\n",
    "        ):\n",
    "            if isinstance(regressor, xgb.XGBRegressor):\n",
    "                regressor_type = \"xgboost\"\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Unsupported regressor type. Supported types: XGBRegressor\"\n",
    "                )\n",
    "\n",
    "            # Use the entire training data for hyperparameter tuning\n",
    "            best_params = hyperparameter_tune_bayesian(X, y, regressor_type)\n",
    "            print(f\"Best hyperparameters for {regressor_type} Fold: {best_params}\")\n",
    "            regressor.set_params(**best_params)\n",
    "\n",
    "        # Split into training and validation folds\n",
    "        X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Fit the model on the training fold\n",
    "        regressor.fit(X_train_fold, np.log1p(y_train_fold))  # Fit on log-transformed target\n",
    "        fold_models.append(regressor)\n",
    "        y_pred_log_fold = regressor.predict(X_val_fold)\n",
    "        y_pred_fold = np.expm1(y_pred_log_fold)  # Convert back to original scale\n",
    "\n",
    "        # Calculate RMSE for the validation fold\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "\n",
    "        if (\n",
    "            isinstance(regressor, (xgb.XGBRegressor))\n",
    "            and tune_hyperparameters\n",
    "        ):\n",
    "            print(model_name, f\"RMSE: {rmse:.4f}, Train Size: {len(y_train_fold)}, Test Size: {len(y_val_fold)}\")\n",
    "\n",
    "        # if regular xgboost, then print rmse for best folds since this is a high performing model\n",
    "        elif isinstance(regressor, RandomForestRegressor):\n",
    "            print(f\"Random Forest: {rmse:.4f}, Train Size: {len(y_train_fold)}, Test Size: {len(y_val_fold)}\")\n",
    "\n",
    "        rmse_scores.append(rmse)\n",
    "        train_sizes.append(len(y_train_fold))\n",
    "        test_sizes.append(len(y_val_fold))\n",
    "\n",
    "        # Record results for 'Id', 'Actual', 'Predicted', 'Fold', and 'Set' in a list\n",
    "        fold_results.append({\n",
    "            'Id' : foldCount,\n",
    "            'Actual': y_val_fold.tolist(),\n",
    "            'Predicted': y_pred_fold,\n",
    "            'rmse' : rmse\n",
    "        })\n",
    "        foldCount += 1\n",
    "\n",
    "        \n",
    "    # Create a DataFrame from the results\n",
    "    result_df = pd.DataFrame(\n",
    "        fold_results, columns=[\"Id\", \"Actual\", \"Predicted\", 'rmse']\n",
    "    )\n",
    "\n",
    "    return result_df, rmse_scores, train_sizes, test_sizes, fold_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, `compute_rmse_std_k_fold`, calculates the mean and standard deviation of RMSE scores from k-fold cross-validation on a given DataFrame using a specified model. It optionally tunes hyperparameters for the model and appends the RMSE scores from each fold to a list. The function returns the mean and standard deviation of the RMSE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_std_k_fold(df, model, tune_hyper=False, model_name=None):\n",
    "    rmse_list = []\n",
    "        \n",
    "    rmse_list.append(k_fold_regression(df, model, tune_hyperparameters=tune_hyper, model_name=model_name)[1])\n",
    "\n",
    "    mean = np.mean(rmse_list)\n",
    "    std = np.std(rmse_list)\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are training each of the models and saving their RMSE and STD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the avg rmse and std over 30 tests for each model\n",
    "lr_no_int_list = compute_rmse_std_k_fold(nba_df, lr_no_int)\n",
    "lr_w_int_list = compute_rmse_std_k_fold(nba_df, lr_w_int)\n",
    "elastic_net_list = compute_rmse_std_k_fold(nba_df, elastic_net)\n",
    "dt_list = compute_rmse_std_k_fold(nba_df, dt)\n",
    "rf_list = compute_rmse_std_k_fold(nba_df, rf)\n",
    "model_xgb_list = compute_rmse_std_k_fold(nba_df, model_xgb)\n",
    "\n",
    "#The next line takes a while (Roughly 10 mins), If want to quickly run, comment out this line and the line below in data\n",
    "model_xgb_hyper_list = compute_rmse_std_k_fold(nba_df, model_xgb, tune_hyper=True)\n",
    "\n",
    "# plot RMSE and STD for each Algorithm\n",
    "data = {\n",
    "    \"Linear (No Intercept)\": lr_no_int_list,\n",
    "    \"Linear (w/ Intercept)\": lr_w_int_list,\n",
    "    \"Elastic Net\": elastic_net_list,\n",
    "    \"Decision Tree\": dt_list,\n",
    "    \"Random Forest\": rf_list,\n",
    "    \"XGBoost\": model_xgb_list,\n",
    "\n",
    "    #Comment below if want to run quicker\n",
    "    \"XGBoost Hyper\": model_xgb_hyper_list,\n",
    "}\n",
    "data_df = pd.DataFrame(data=data).T.reset_index().sort_values(by=[0], ascending=True)\n",
    "data_df.columns = [\"Algorithm\", \"RMSE\", \"STD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the bar plot\n",
    "data_df.plot(kind=\"bar\", x=\"Algorithm\", y=[\"RMSE\", \"STD\"], figsize=(20, 10), rot=0)\n",
    "plt.xlabel(\"Algorithm\", fontsize=20)\n",
    "plt.ylabel(\"Root Mean Squared Error / Standard Deviation\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper tuning XGBoost did not outperform our regular XGBoost model. Therefore, we will not be using it in our stacked or voting models going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Use Stacked Models and Voting Models to Create a Meta Model\n",
    "\n",
    "Stacked models and voting models are ensemble techniques that combine the predictions of multiple base models to improve overall performance. Here’s why they are beneficial:\n",
    "\n",
    "1. **Leverage Strengths of Multiple Models:**\n",
    "   - Different models have different strengths and weaknesses. By combining them, you can take advantage of the strengths of each model while mitigating their weaknesses. For instance, decision trees can handle non-linear relationships well, while linear models can be more stable with fewer parameters.\n",
    "\n",
    "2. **Reduce Overfitting:**\n",
    "   - Individual models might overfit the training data, but combining multiple models can reduce this risk. The errors of individual models may cancel each other out, leading to a more generalizable model.\n",
    "\n",
    "3. **Improve Prediction Accuracy:**\n",
    "   - Ensemble methods often achieve better performance than individual models. By aggregating the predictions of multiple models, the overall prediction accuracy is typically improved.\n",
    "\n",
    "4. **Model Robustness:**\n",
    "   - Ensemble methods can provide more robust predictions. If one model performs poorly on certain data points, the other models can compensate, leading to more stable and reliable predictions.\n",
    "\n",
    "### Stacking Regressor\n",
    "\n",
    "A stacking regressor combines the predictions of several base models using another model (the final estimator) to make the final prediction. This method can capture complex patterns in the data by leveraging the diverse learning algorithms of the base models.\n",
    "\n",
    "### Voting Regressor\n",
    "\n",
    "A voting regressor combines the predictions of multiple models by averaging their predictions (or using weighted averaging). This method is simple yet effective in combining the strengths of multiple models to achieve better overall performance.\n",
    "\n",
    "### Combined Stacking and Voting Regressor\n",
    "\n",
    "Combining stacking and voting regressors further enhances the model’s ability to generalize and improve prediction accuracy. The stacking regressor uses the voting regressor as its final estimator, combining the strengths of both methods to create a powerful meta-model.\n",
    "\n",
    "By using stacked models and voting models, you aim to create a meta-model that is more accurate, robust, and capable of generalizing better to new data compared to individual models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# first stacking model\n",
    "  \n",
    "estimators = [\n",
    "   ('decision_tree', dt),\n",
    "   ('rf', rf),\n",
    "]\n",
    "\n",
    "\n",
    "sr = StackingRegressor(\n",
    "   estimators=estimators,\n",
    "   final_estimator=model_xgb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# voting stacking model, putting weights on different models\n",
    "\n",
    "vr = VotingRegressor([\n",
    "   ('rf', rf),\n",
    "   ('model_xgb', model_xgb),\n",
    "   ('decision_tree', dt),\n",
    "  \n",
    "], weights=[1,1,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators2 = [\n",
    "   ('rf', rf),\n",
    "   ('model_xgb', model_xgb),\n",
    "   #('decision_tree', dt)\n",
    "]\n",
    "\n",
    "# using the voting model as our final estimator\n",
    "\n",
    "sr2 = StackingRegressor(\n",
    "   estimators=estimators2,\n",
    "   final_estimator=vr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More tesing with the new models\n",
    "Removed Nearest Neighbor, Linear No Int, Linear W Int, Elastic Net, and Descision tree because they are worst performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_list = compute_rmse_std_k_fold(nba_df, rf)\n",
    "model_xgb_list = compute_rmse_std_k_fold(nba_df, model_xgb)\n",
    "dt_list = compute_rmse_std_k_fold(nba_df, dt)\n",
    "\n",
    "sr_list = compute_rmse_std_k_fold(nba_df, sr)\n",
    "vr_list = compute_rmse_std_k_fold(nba_df, vr)\n",
    "sr2_list = compute_rmse_std_k_fold(nba_df, sr2)\n",
    "\n",
    "#model_xgb_hyper_list = compute_rmse_std_k_fold(nba_df, model_xgb, tune_hyper=True)\n",
    "\n",
    "# plot RMSE and STD for each Algorithm\n",
    "data = {\n",
    "    \"Random Forest\": rf_list,\n",
    "    \"XGBoost\": model_xgb_list,\n",
    "    \"Decision Tree\": dt_list,\n",
    "    \n",
    "    \"Stacking Regressor\": sr_list,\n",
    "    \"Voting Regressor\": vr_list,\n",
    "    \"Stacking Regressor 2\": sr2_list,\n",
    "    \n",
    "    #\"XGBoost Hyper\": model_xgb_hyper_list,\n",
    "}\n",
    "data_df = pd.DataFrame(data=data).T.reset_index().sort_values(by=[0], ascending=True)\n",
    "data_df.columns = [\"Algorithm\", \"RMSE\", \"STD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the bar plot\n",
    "data_df.plot(kind=\"bar\", x=\"Algorithm\", y=[\"RMSE\", \"STD\"], figsize=(20, 10), rot=0)\n",
    "plt.xlabel(\"Algorithm\", fontsize=20)\n",
    "plt.ylabel(\"Root Mean Squared Error / Standard Deviation\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like Random Forest is still our best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the lowest rmse fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_list = k_fold_regression(nba_df, rf, model_name=\"Random Forest\")\n",
    "rf_preds = rf_list[0]\n",
    "rf_models = rf_list[4]\n",
    "\n",
    "rf_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by the 'rmse' column in ascending order\n",
    "sorted_df = rf_preds.sort_values(by='rmse', ascending=True)\n",
    "\n",
    "# Grab the first row of the sorted DataFrame\n",
    "first_row = sorted_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_row[[\"Actual\", \"Predicted\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming first_row is the output from the previous steps\n",
    "actual_values = first_row[\"Actual\"]\n",
    "predicted_values = first_row[\"Predicted\"]\n",
    "\n",
    "# Plot the density plots for Actual and Predicted values\n",
    "sns.kdeplot(\n",
    "    data=actual_values,\n",
    "    fill=True,\n",
    "    common_norm=False,\n",
    "    alpha=0.4,\n",
    "    label=\"Actual\"\n",
    ")\n",
    "sns.kdeplot(\n",
    "    data=predicted_values,\n",
    "    fill=True,\n",
    "    common_norm=False,\n",
    "    alpha=0.4,\n",
    "    label=\"Predicted\"\n",
    ")\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim((0, 700000))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute difference and create a new column\n",
    "first_row['Difference'] = first_row['Actual'] - first_row['Predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check on predictions\n",
    "print(\"min prediction: \", first_row[\"Predicted\"].min())\n",
    "print(\"max prediction: \", first_row[\"Predicted\"].max())\n",
    "print(\"max error: \", first_row[\"Difference\"].max())\n",
    "print(\"mean error: \", abs(first_row[\"Difference\"]).mean())\n",
    "print(\"median error: \", np.median(abs(first_row[\"Difference\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestdiffs = pd.Series(first_row['Difference'])\n",
    "# plot a histogram of the difference of our actuals and predictions\n",
    "bestdiffs.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is a large outlier we are off by (~15,000,000). Let's examine this record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_values = bestdiffs[bestdiffs > 10000000]\n",
    "\n",
    "# Print the filtered Series\n",
    "print(\"Here is the low outlier value: \" + str(filtered_values.iloc[0]))\n",
    "\n",
    "# Get the low outlier value\n",
    "low_outlier_value = filtered_values.iloc[0]\n",
    "\n",
    "# Find the index of 1 in first_row\n",
    "index = np.where(first_row['Difference'] == low_outlier_value)[0][0]\n",
    "# Get the corresponding 'Actual' value from first_row using boolean indexing\n",
    "corresponding_actual = first_row['Actual'][index]\n",
    "\n",
    "# Print the corresponding 'Actual' value\n",
    "print(\"The 'Actual' value corresponding to the outlier value is:\", corresponding_actual)\n",
    "\n",
    "corresponding_Predicted = first_row['Predicted'][index]\n",
    "\n",
    "# Print the corresponding 'Actual' value\n",
    "print(\"The 'Predicted' value corresponding to the outlier value is:\", corresponding_Predicted)\n",
    "\n",
    "nba_df[nba_df['Salary'] == corresponding_actual]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine this record to see if there is something strange about it, but in your notebook you didn't so we'll stop here. For our purposes, we see our model performs well and our predicted and actual distributions are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance Plot\n",
    "\n",
    "Tree based models (Decision Trees, Random Forest, GBMs) have feature importance plots that allow you to see which features have the most impact on our model. Let's take a look at our Random Forest model that we used in our meta-model to get a sense of which features are the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names\n",
    "feature_names = nba_df.columns.drop(['Salary', 'Id', 'PlayerName', 'Salary_normalized'])\n",
    "\n",
    "# Get the first model from the list\n",
    "model = rf_models[0]\n",
    "\n",
    "# Get the feature importances of the first model\n",
    "feature_importances = model.feature_importances_\n",
    "sorted_indices = feature_importances.argsort()[::-1]\n",
    "\n",
    "# Create a single plot for the first model\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.bar(range(len(feature_importances)), feature_importances[sorted_indices])\n",
    "ax.set_xticks(range(len(feature_importances)))\n",
    "ax.set_xticklabels(feature_names[sorted_indices], rotation=90)\n",
    "ax.set_xlabel(\"Features\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set_title(\"Variable Importance - Random Forest\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really odd because Random Forest seems to be favoring one feature way more than the others. To compare we will also create a plot for our XGBoost model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_important = model_xgb.get_booster().get_score(importance_type=\"weight\")\n",
    "\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(\n",
    "    by=\"score\", ascending=False\n",
    ")\n",
    "data[:20].plot(kind=\"barh\", figsize=(20, 10)).invert_yaxis()\n",
    "## plot top 20 features\n",
    "plt.xlabel(\"Feature Importance\", fontsize=20)\n",
    "plt.ylabel(\"Feature Name\", fontsize=20)\n",
    "plt.title(\"Variable Importance - XGBoost\", fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ccb5ea80edbfae8c6780fa0b5909321291a27faa1189b515049edc570dffb73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
